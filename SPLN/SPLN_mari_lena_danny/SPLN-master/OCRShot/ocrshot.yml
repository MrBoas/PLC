---
definicoes:
  path: #/home/<username>/Imagens
metadados: [
  "Título",
  "Data",
  Classe: [
    "jornal_html",
    "resumo",
    "novo_acordo",
    "polaridade"
  ]
]
funcoes:
  resumo: |-
    from string import punctuation
    import nltk
    from collections import defaultdict

    frases = nltk.sent_tokenize(text)
    palavras = nltk.word_tokenize(text.lower())

    palavras = [palavra for palavra in palavras if palavra not in nltk.corpus.stopwords.words('portuguese') and not re.match('\p{punct}', palavra)]

    frequencia = nltk.FreqDist(palavras)

    dictionary = defaultdict(int)

    for i, frase in enumerate(frases):
      for palavra in nltk.word_tokenize(frase.lower()):
        if palavra in frequencia:
          dictionary[i] = dictionary.get(i, 0) + frequencia[palavra]

    idxs = sorted(dictionary.items(), key = lambda kv: kv[1])[::-1][:4]

    for i, v in sorted(idxs):
      print(frases[i])
  jornal_html: |-
    output = "<p>" + re.sub(r'\n\n', r'</p></p>', text) + "</p>"
    with open('out.html', 'w') as f:
      f.write("<header>")
      f.write("<title>" + metadata["Título"] + "</title>")
      f.write("<link rel=\"stylesheet\" type=\"text/css\" href=\"data/mystyle.css\">")
      f.write("<link href=\"https://fonts.googleapis.com/css?family=Roboto\" rel=\"stylesheet\"")
      f.write("</header")
      f.write("<body>")
      f.write("<div class=\"header\">")
      f.write("<h1>" + metadata["Título"] + "</h1>")
      f.write("<h3> Publicado a " + metadata["Data"] + "</h3>")
      f.write("</div>")
      f.write("<div class=\"wrapper\">")
      f.write("<p>" + output + "</p>")
      f.write("</div>")
      f.write("</body>")
  novo_acordo: |-
      from bs4 import BeautifulSoup
      import requests
      import regex as re
      import fileinput
      import string

      urlBase = "http://www.portaldalinguaportuguesa.org/novoacordo.php?action=novoacordo&act=list&letter={}&version=pe"
      letters = [c for c in string.ascii_lowercase[0:]]

      dic = {}

      for letter in letters:
        url = urlBase.format(letter)
        response = requests.get(url).content
        soup = BeautifulSoup(response, "html.parser")
        table = soup.find("table", id='rollovertable').find_all('tr')[0]
        td = table.find("td")
        if td:
          palavras = td.getText(separator=u'##')
          palavras = re.split(r'\n', palavras.strip())
          for pal in palavras:
            ant, dep = tuple(re.findall(r'(?<=\#)[^#,]+', pal)[:2])
            dic[ant] = dep

      def corrige_palavra(match):
        pal = match[0]
        c = pal[0]
        pal = dic.get(pal.lower(), pal)
        lista = list(pal)
        lista[0] = c
        return "".join(lista)

      output = re.sub(r'\p{L}+', corrige_palavra, text)
      print(output)
  polaridade: |-
      import regex as re

      dict = {}

      def build_dict():
        for line in open('lexicos/lexico_v3.2.txt', 'r'):
          a = re.split(r'[,]+', line)
          dict[a[0]] = int(a[2])
        for line in open('lexicos/SentiLex-flex-PT02.txt', 'r'):
          a = re.findall(r'([^\n\.;]+),([^\n\.;]+).PoS=([A-Za-z]+);[^\n;]*;[^\n;]*;POL:N.=(-1|0|1).*',line)
          a = a[0]
          dict[a[0]] = int(a[3])

      def polarity():
          value = 0
          found = 0
          lines = re.split(r'[\p{punct}\s]+', text.lower())
          for p in lines:
              if p in dict:
                  found += 1
                  value += dict.get(p,0)
          if found:
            print("Analisadas " + str(found) + " palavras.\nPolaridade: {0:.3f}".format(round(value/found,3)))
          else:
            print("Não foi possível analisar!!!")

      build_dict()
      polarity()
